{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below libraries and functions allow us to define our own schema\n",
    "from pyspark.sql.types import StructField, StructType,IntegerType,StringType,DateType,DecimalType,BooleanType\n",
    "from pyspark.sql.functions import col, when, sum, avg, row_number\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Craeting the Spark Session\n",
    "spark = SparkSession.builder.appName('ipl-data-analysis').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the Data from the AWS S3 bucket\n",
    "\n",
    "# First Dataset out of 5 \n",
    "\n",
    "# The option keyword here allows us to change the dataset or infer things while reading them (examples below)\n",
    "# Option 1: makes the first row headers\n",
    "# Option 2: infers the schema of the data from the dataset this will help correctly classify the type of data in each column\n",
    "\n",
    "ball_df = spark.read.format('csv').option('header','true').option('inferSchema').load('s3://ipl-data-analysis-project/Ball_By_Ball.csv')\n",
    "\n",
    "# HOWEVER, it is best practice to create your OWN schema because inferring could be wrong\n",
    "\n",
    "# DO NOT RUN the above line with the below lines. I did this to remind me of Spark terminology\n",
    "\n",
    "# This way we define each column however we deem the data to be.\n",
    "\n",
    "ball_schema = StructType([\n",
    "    StructField(\"match_id\", IntegerType(), True),\n",
    "    StructField(\"over_id\", IntegerType(), True),\n",
    "    StructField(\"ball_id\", IntegerType(), True),\n",
    "    StructField(\"innings_no\", IntegerType(), True),\n",
    "    StructField(\"team_batting\", StringType(), True),\n",
    "    StructField(\"team_bowling\", StringType(), True),\n",
    "    StructField(\"striker_batting_position\", IntegerType(), True),\n",
    "    StructField(\"extra_type\", StringType(), True),\n",
    "    StructField(\"runs_scored\", IntegerType(), True),\n",
    "    StructField(\"extra_runs\", IntegerType(), True),\n",
    "    StructField(\"wides\", IntegerType(), True),\n",
    "    StructField(\"legbyes\", IntegerType(), True),\n",
    "    StructField(\"byes\", IntegerType(), True),\n",
    "    StructField(\"noballs\", IntegerType(), True),\n",
    "    StructField(\"penalty\", IntegerType(), True),\n",
    "    StructField(\"bowler_extras\", IntegerType(), True),\n",
    "    StructField(\"out_type\", StringType(), True),\n",
    "    StructField(\"caught\", BooleanType(), True),\n",
    "    StructField(\"bowled\", BooleanType(), True),\n",
    "    StructField(\"run_out\", BooleanType(), True),\n",
    "    StructField(\"lbw\", BooleanType(), True),\n",
    "    StructField(\"retired_hurt\", BooleanType(), True),\n",
    "    StructField(\"stumped\", BooleanType(), True),\n",
    "    StructField(\"caught_and_bowled\", BooleanType(), True),\n",
    "    StructField(\"hit_wicket\", BooleanType(), True),\n",
    "    StructField(\"obstructingfeild\", BooleanType(), True),\n",
    "    StructField(\"bowler_wicket\", BooleanType(), True),\n",
    "    StructField(\"match_date\", DateType(), True),\n",
    "    StructField(\"season\", IntegerType(), True),\n",
    "    StructField(\"striker\", IntegerType(), True),\n",
    "    StructField(\"non_striker\", IntegerType(), True),\n",
    "    StructField(\"bowler\", IntegerType(), True),\n",
    "    StructField(\"player_out\", IntegerType(), True),\n",
    "    StructField(\"fielders\", IntegerType(), True),\n",
    "    StructField(\"striker_match_sk\", IntegerType(), True),\n",
    "    StructField(\"strikersk\", IntegerType(), True),\n",
    "    StructField(\"nonstriker_match_sk\", IntegerType(), True),\n",
    "    StructField(\"nonstriker_sk\", IntegerType(), True),\n",
    "    StructField(\"fielder_match_sk\", IntegerType(), True),\n",
    "    StructField(\"fielder_sk\", IntegerType(), True),\n",
    "    StructField(\"bowler_match_sk\", IntegerType(), True),\n",
    "    StructField(\"bowler_sk\", IntegerType(), True),\n",
    "    StructField(\"playerout_match_sk\", IntegerType(), True),\n",
    "    StructField(\"battingteam_sk\", IntegerType(), True),\n",
    "    StructField(\"bowlingteam_sk\", IntegerType(), True),\n",
    "    StructField(\"keeper_catch\", BooleanType(), True),\n",
    "    StructField(\"player_out_sk\", IntegerType(), True),\n",
    "    StructField(\"matchdatesk\", DateType(), True)\n",
    "])\n",
    "\n",
    "# Now we can run the same line intially when creating the df but with a small twist as we already defined the schema above:\n",
    "\n",
    "ball_df = spark.read.format('csv').schema(ball_schema).option('header', 'true').load('s3://ipl-data-analysis-project/Ball_By_Ball.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I want to do the same thing for each csv file stored in the AWS S3 bucket\n",
    "\n",
    "# Match Data\n",
    "match_schema = StructType([\n",
    "    StructField(\"match_sk\", IntegerType(), True),\n",
    "    StructField(\"match_id\", IntegerType(), True),\n",
    "    StructField(\"team1\", StringType(), True),\n",
    "    StructField(\"team2\", StringType(), True),\n",
    "    StructField(\"match_date\", DateType(), True),\n",
    "    StructField(\"season_year\", IntegerType(), True),\n",
    "    StructField(\"venue_name\", StringType(), True),\n",
    "    StructField(\"city_name\", StringType(), True),\n",
    "    StructField(\"country_name\", StringType(), True),\n",
    "    StructField(\"toss_winner\", StringType(), True),\n",
    "    StructField(\"match_winner\", StringType(), True),\n",
    "    StructField(\"toss_name\", StringType(), True),\n",
    "    StructField(\"win_type\", StringType(), True),\n",
    "    StructField(\"outcome_type\", StringType(), True),\n",
    "    StructField(\"manofmach\", StringType(), True),\n",
    "    StructField(\"win_margin\", IntegerType(), True),\n",
    "    StructField(\"country_id\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "match_df = spark.read.schema(match_schema).format(\"csv\").option(\"header\",\"true\").load(\"s3://ipl-data-analysis-project/Match.csv\")\n",
    "\n",
    "#Player Data\n",
    "player_schema = StructType([\n",
    "    StructField(\"player_sk\", IntegerType(), True),\n",
    "    StructField(\"player_id\", IntegerType(), True),\n",
    "    StructField(\"player_name\", StringType(), True),\n",
    "    StructField(\"dob\", DateType(), True),\n",
    "    StructField(\"batting_hand\", StringType(), True),\n",
    "    StructField(\"bowling_skill\", StringType(), True),\n",
    "    StructField(\"country_name\", StringType(), True)\n",
    "])\n",
    "\n",
    "player_df = spark.read.schema(player_schema).format(\"csv\").option(\"header\",\"true\").load(\"s3://ipl-data-analysis-project/Player.csv\")\n",
    "\n",
    "#Player-Match Data\n",
    "player_match_schema = StructType([\n",
    "    StructField(\"player_match_sk\", IntegerType(), True),\n",
    "    StructField(\"playermatch_key\", DecimalType(), True),\n",
    "    StructField(\"match_id\", IntegerType(), True),\n",
    "    StructField(\"player_id\", IntegerType(), True),\n",
    "    StructField(\"player_name\", StringType(), True),\n",
    "    StructField(\"dob\", DateType(), True),\n",
    "    StructField(\"batting_hand\", StringType(), True),\n",
    "    StructField(\"bowling_skill\", StringType(), True),\n",
    "    StructField(\"country_name\", StringType(), True),\n",
    "    StructField(\"role_desc\", StringType(), True),\n",
    "    StructField(\"player_team\", StringType(), True),\n",
    "    StructField(\"opposit_team\", StringType(), True),\n",
    "    StructField(\"season_year\", IntegerType(), True),\n",
    "    StructField(\"is_manofthematch\", BooleanType(), True),\n",
    "    StructField(\"age_as_on_match\", IntegerType(), True),\n",
    "    StructField(\"isplayers_team_won\", BooleanType(), True),\n",
    "    StructField(\"batting_status\", StringType(), True),\n",
    "    StructField(\"bowling_status\", StringType(), True),\n",
    "    StructField(\"player_captain\", StringType(), True),\n",
    "    StructField(\"opposit_captain\", StringType(), True),\n",
    "    StructField(\"player_keeper\", StringType(), True),\n",
    "    StructField(\"opposit_keeper\", StringType(), True)\n",
    "])\n",
    "\n",
    "player_match_df = spark.read.schema(player_match_schema).format(\"csv\").option(\"header\",\"true\").load(\"s3://ipl-data-analysis-project/Player_match.csv\")\n",
    "\n",
    "team_schema = StructType([\n",
    "    StructField(\"team_sk\", IntegerType(), True),\n",
    "    StructField(\"team_id\", IntegerType(), True),\n",
    "    StructField(\"team_name\", StringType(), True)\n",
    "])\n",
    "\n",
    "team_df = spark.read.schema(team_schema).format(\"csv\").option(\"header\",\"true\").load(\"s3://ipl-data-analysis-project/Team.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation Spark! \n",
    "# Spark runs on the lazy principle, meaning the outcome of each line below will not immediately show, instead it will wait until all transformation code is written and run it then efficiently.\n",
    "\n",
    "# Example: Let's filter to only show valid deliviries in Cricket Matches (no noballs and no wides)\n",
    "ball_df = ball_df.filter((col('wides')== 0) & (col('noballs') == 0))\n",
    "\n",
    "# Example: Total and Average Runs Scored in Each Match and Inning. \n",
    "total_and_avg_runs  = ball_df.groupBy('matchid','innings_no').agg(\n",
    "    sum('runs_scored').alias('total_runs'),\n",
    "    avg('runs_scored').alias('avg_runs')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL Window Function Example\n",
    "windowSpec = Window.partitionBy('matchid','innings_no').orderBy('over_id')\n",
    "\n",
    "ball_df - ball_df.withColumn(\n",
    "    'running_total_runs',\n",
    "    sum('runs_scored').over(windowSpec)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Conditional Formatting : flag for high impact balls (wicket OR more than 6 runs including extras)\n",
    "\n",
    "# Explanation: this function will check when columns runs scored and extra runs are bigger than 6 or if there was a wicket THEN return value of TRUE otherwise return FALSE\n",
    "ball_df = ball_df.withColumn(\n",
    "    'high_impact',\n",
    "    when((col('runs_scored') + col('extra_runs') > 6) | (col('bowler_wicket') == True), True).otherwise(False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysparl.sql.functions import year,month,dayofmonth,when\n",
    "\n",
    "# Time Analysis - Splitting the Match Date to Year, Month, and Day of Month\n",
    "\n",
    "match_df = match_df.withColumn('year', year('match_date'))\n",
    "match_df = match_df.withColumn('month', month('match_date'))\n",
    "match_df = match_df.withColumn('day', dayofmonth('match_date'))\n",
    "\n",
    "# Example Analysis: Identifying Matches with a High, Medium, and Low Margin Wins. \n",
    "\n",
    "match_df.match_df.withColumn(\n",
    "    'win_margin_type',\n",
    "    when(col('win_margin') >= 100, 'High')\n",
    "    .when((col('win_margin') >= 50) & (col('win_margin') < 100), 'Medium')\n",
    "    .otherwise('Low')\n",
    ")\n",
    "\n",
    "# Analyze Teams's wins based on the Toss.\n",
    "\n",
    "match_df = match_df.withColumn(\n",
    "    'toss_match_winner',\n",
    "    when(col('toss_winner') == col('match_winner'), 'Yes').otherwise('No')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, regexp_replace\n",
    "\n",
    "# Normalizing the Data for the Player Names, removing all unnecessasry symbols, signs and more\n",
    "\n",
    "player_df = player.df.withColumn('player_name', lower(regexp_replace('player_name ', '[^a-zA-Z0-9]', \"\")))\n",
    "\n",
    "# Any Missing Values for 'batting_hand' and 'bowling_skill' to fill with 'unknown'\n",
    "\n",
    "player_df = player_df.na.fill({'batting_hand': 'unknown', 'bowling_skill': 'unknown'})\n",
    "\n",
    "# Separting players with left-handed batting style from those that are right-handed\n",
    "\n",
    "player_df = player_df.withColumn(\n",
    "    'batting_style',\n",
    "    when(col('batting_hand').contains('Left'), 'Left-handed')\n",
    "    .when(col('batting_hand').contains('Right'), 'Right-handed')\n",
    "    .otherwise('unknown')\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, current_date, expr\n",
    "\n",
    "# Assign a veteran status to players aged 35+ \n",
    "\n",
    "player_match_df = player_match_df.withColumn(\n",
    "    'veteran_status',\n",
    "    when(col('age_as_on_match') >= 35, 'Veteran').otherwise('Non-Veteran')\n",
    ")\n",
    "\n",
    "# Filter the dataset to only include players that batted (played the game)\n",
    "\n",
    "player_match_df = player_match_df.filter(col('batting_status') != 'Did Not Bat')\n",
    "\n",
    "# Column to Calculate the Player year count since their initial debut \n",
    "\n",
    "player_match_df = player_match_df.withColumn(\n",
    "    'years_since_debut',\n",
    "    (year(current_date()) - col('season_year'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Temporary SQL tables\n",
    "ball_df.createOrReplaceTempView('ball_table')\n",
    "match_df.createOrReplaceTempView(\"match\")\n",
    "player_df.createOrReplaceTempView(\"player\")\n",
    "player_match_df.createOrReplaceTempView(\"player_match\")\n",
    "team_df.createOrReplaceTempView(\"team\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Actual SQL queries in Pyspark \n",
    "\n",
    "top_scoring_batsmen_per_season = spark.sql(\"\"\"\n",
    "SELECT \n",
    "p.player_name,\n",
    "m.season_year,\n",
    "SUM(b.runs_scored) AS total_runs \n",
    "\n",
    "FROM \n",
    "    ball_by_ball b\n",
    "\n",
    "JOIN \n",
    "    match m ON b.match_id = m.match_id   \n",
    "\n",
    "JOIN \n",
    "    player_match pm ON m.match_id = pm.match_id AND b.striker = pm.player_id     \n",
    "\n",
    "JOIN \n",
    "    player p ON p.player_id = pm.player_id\n",
    "\n",
    "GROUP BY \n",
    "    p.player_name, m.season_year\n",
    "\n",
    "ORDER BY \n",
    "    m.season_year, total_runs DESC\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another Actual SQL query\n",
    "\n",
    "economical_bowlers_powerplay = spark.sql(\"\"\"\n",
    "SELECT \n",
    "p.player_name, \n",
    "AVG(b.runs_scored) AS avg_runs_per_ball, \n",
    "COUNT(b.bowler_wicket) AS total_wickets\n",
    "\n",
    "FROM \n",
    "    ball_by_ball b\n",
    "JOIN \n",
    "    player_match pm ON b.match_id = pm.match_id AND b.bowler = pm.player_id\n",
    "JOIN \n",
    "    player p ON pm.player_id = p.player_id\n",
    "WHERE \n",
    "    b.over_id <= 6\n",
    "GROUP BY \n",
    "    p.player_name\n",
    "HAVING \n",
    "    COUNT(*) >= 1\n",
    "ORDER BY \n",
    "    avg_runs_per_ball, total_wickets DESC\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cointoss_impact_matches = spark.sql(\"\"\"\n",
    "SELECT m.match_id, m.toss_winner, m.toss_name, m.match_winner,\n",
    "       CASE WHEN m.toss_winner = m.match_winner THEN 'Won' ELSE 'Lost' END AS match_outcome\n",
    "FROM \n",
    "    match m\n",
    "WHERE \n",
    "    m.toss_name IS NOT NULL\n",
    "ORDER BY \n",
    "    m.match_id\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_runs_in_wins = spark.sql(\"\"\"\n",
    "SELECT p.player_name, AVG(b.runs_scored) AS avg_runs_in_wins, COUNT(*) AS innings_played\n",
    "\n",
    "FROM \n",
    "    ball_by_ball b\n",
    "JOIN \n",
    "    player_match pm ON b.match_id = pm.match_id AND b.striker = pm.player_id\n",
    "JOIN \n",
    "    player p ON pm.player_id = p.player_id\n",
    "JOIN \n",
    "    match m ON pm.match_id = m.match_id\n",
    "WHERE \n",
    "    m.match_winner = pm.player_team\n",
    "GROUP BY \n",
    "    p.player_name\n",
    "ORDER BY \n",
    "    avg_runs_in_wins ASC\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An Example of A Simple Visualization Using matplot \n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'economical_bowlers_powerplay' is already executed and available as a Spark DataFrame\n",
    "economical_bowlers_pd = economical_bowlers_powerplay.toPandas()\n",
    "\n",
    "# Visualizing using Matplotlib\n",
    "plt.figure(figsize=(12, 8))\n",
    "# Limiting to top 10 for clarity in the plot\n",
    "top_economical_bowlers = economical_bowlers_pd.nsmallest(10, 'avg_runs_per_ball')\n",
    "plt.bar(top_economical_bowlers['player_name'], top_economical_bowlers['avg_runs_per_ball'], color='skyblue')\n",
    "plt.xlabel('Bowler Name')\n",
    "plt.ylabel('Average Runs per Ball')\n",
    "plt.title('Most Economical Bowlers in Powerplay Overs (Top 10)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
